import torch
import json
import os
# é€‚é…æ–‡ä»¶ç»“æ„ï¼šä½ç§©æ¨¡å‹ï¼ˆattentions.pyï¼‰
from attentions import Encoder, MultiHeadAttention, LowRankLinear
from models import TextEncoder, SynthesizerTrn

# ===================== é…ç½®å‚æ•°ï¼ˆä»…éœ€ä¿®æ”¹è¿™4ä¸ªè·¯å¾„ï¼Œå…¶ä½™é»˜è®¤ï¼‰ =====================
ORIGINAL_CKPT_PATH = "./pretrained_ljs.pth"  # åŸå§‹é¢„è®­ç»ƒæƒé‡è·¯å¾„
NEW_INIT_CKPT_PATH = "./init_low_rank_model.pth"  # è¿ç§»åæƒé‡ä¿å­˜è·¯å¾„
CONFIG_PATH = "./configs/ljs_base.json"  # ä½ çš„é…ç½®æ–‡ä»¶è·¯å¾„
RANK = 32  # ä½ç§©åˆ†è§£çš„ç§©ï¼Œå’Œattentions.pyä¸­ä¸€è‡´
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===================== æ ¸å¿ƒå‡½æ•°ï¼šå·ç§¯æƒé‡ -> ä½ç§©çº¿æ€§å±‚æƒé‡è¿ç§» =====================
def svd_weight_transfer(conv_weight, in_features, out_features, r):
    """
    é€šè¿‡SVDå¥‡å¼‚å€¼åˆ†è§£ï¼Œå°†1x1å·ç§¯æƒé‡æ˜ å°„åˆ°ä½ç§©çº¿æ€§å±‚
    :param conv_weight: åŸå·ç§¯å±‚æƒé‡ï¼Œshape [out_channels, in_channels, kernel_size]
    :param in_features: ä½ç§©å±‚è¾“å…¥ç‰¹å¾æ•°
    :param out_features: ä½ç§©å±‚è¾“å‡ºç‰¹å¾æ•°
    :param r: ä½ç§©åˆ†è§£çš„ç§©
    :return: W1_init (in_features, r), W2_init (r, out_features) ï¼ˆä¿®æ­£åç»´åº¦ï¼‰
    """
    # å»é™¤å·ç§¯æ ¸ç»´åº¦ï¼ˆ1x1å·ç§¯ï¼Œè½¬ä¸º2Dæƒé‡ï¼‰
    conv_weight_2d = conv_weight.squeeze(-1)  # [out_channels, in_channels] -> [C, C]ï¼ˆæ­¤å¤„C=192ï¼‰
    assert conv_weight_2d.shape == (out_features, in_features), f"æƒé‡å½¢çŠ¶ä¸åŒ¹é…ï¼Œé¢„æœŸ({out_features},{in_features})ï¼Œå®é™…{conv_weight_2d.shape}"

    # æ‰§è¡ŒSVDå¥‡å¼‚å€¼åˆ†è§£
    U, S, V = torch.svd(conv_weight_2d.cpu())
    r = min(r, in_features, out_features)  # ç¡®ä¿ç§©ä¸è¶…è¿‡ç‰¹å¾æ•°

    # å–å‰rä¸ªå¥‡å¼‚å€¼å’Œå‘é‡ï¼Œæ„å»ºä½ç§©æƒé‡
    U_r = U[:, :r]  # [C, r] -> [192, 32]
    S_r = torch.diag(torch.sqrt(S[:r]))  # [r, r] -> [32, 32]
    V_r = V[:, :r]  # [C, r] -> [192, 32]

    # ä¿®æ­£çŸ©é˜µä¹˜æ³•é¡ºåºï¼šç¡®ä¿ç»´åº¦åŒ¹é…
    W1_init = (U_r @ S_r).T  # [C, r] -> [in_features, r]ï¼ˆ[192,32]ï¼‰
    W2_init = (S_r @ V_r.T).T  # [r, C] -> [r, out_features]ï¼ˆ[32,192]ï¼‰

    return W1_init.to(DEVICE), W2_init.to(DEVICE)

def transfer_multihead_attention_weights(original_attn, new_attn, r):
    """
    è¿ç§» MultiHeadAttention ä¸­ conv_q/conv_k/conv_v çš„æƒé‡åˆ°ä½ç§©çº¿æ€§å±‚
    """
    channels = original_attn.channels
    # è¿ç§» conv_q æƒé‡
    q_W1, q_W2 = svd_weight_transfer(original_attn.conv_q.weight, channels, channels, r)
    with torch.no_grad():
        new_attn.conv_q.W1.weight.copy_(q_W1.unsqueeze(-1))  # [r,C]->[r,C,1]
        new_attn.conv_q.W2.weight.copy_(q_W2.unsqueeze(-1))  # [C,r]->[C,r,1]
    # if original_attn.conv_q.bias is not None:
    #     new_attn.conv_q.W2.bias.data = original_attn.conv_q.bias.data.to(DEVICE)

    # è¿ç§» conv_k æƒé‡
    k_W1, k_W2 = svd_weight_transfer(original_attn.conv_k.weight, channels, channels, r)
    with torch.no_grad():
        new_attn.conv_k.W1.weight.copy_(k_W1.unsqueeze(-1))
        new_attn.conv_k.W2.weight.copy_(k_W2.unsqueeze(-1))
    # if original_attn.conv_k.bias is not None:
    #     new_attn.conv_k.W2.bias.data = original_attn.conv_k.bias.data.to(DEVICE)

    # è¿ç§» conv_v æƒé‡
    v_W1, v_W2 = svd_weight_transfer(original_attn.conv_v.weight, channels, channels, r)
    with torch.no_grad():
        new_attn.conv_v.W1.weight.copy_(v_W1.unsqueeze(-1))
        new_attn.conv_v.W2.weight.copy_(v_W2.unsqueeze(-1))
    # if original_attn.conv_v.bias is not None:
    #     new_attn.conv_v.W2.bias.data = original_attn.conv_v.bias.data.to(DEVICE)

    # è¿ç§» conv_o æƒé‡
    new_attn.conv_o.weight.data = original_attn.conv_o.weight.data.to(DEVICE)
    if original_attn.conv_o.bias is not None:
        new_attn.conv_o.bias.data = original_attn.conv_o.bias.data.to(DEVICE)

    # è¿ç§»ç›¸å¯¹ä½ç½®åµŒå…¥æƒé‡
    if original_attn.window_size is not None and hasattr(original_attn, 'emb_rel_k'):
        new_attn.emb_rel_k.data = original_attn.emb_rel_k.data.to(DEVICE)
        new_attn.emb_rel_v.data = original_attn.emb_rel_v.data.to(DEVICE)

    return new_attn

def transfer_encoder_weights(original_encoder, new_encoder, r):
    """
    è¿ç§» Encoder ä¸­æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æƒé‡
    """
    # é€ä¸€å±‚è¿ç§»æ³¨æ„åŠ›æƒé‡
    for i in range(len(original_encoder.attn_layers)):
        original_attn = original_encoder.attn_layers[i]
        new_attn = new_encoder.attn_layers[i]
        new_encoder.attn_layers[i] = transfer_multihead_attention_weights(original_attn, new_attn, r)

    # è¿ç§»å½’ä¸€åŒ–å±‚å’ŒFFNå±‚æƒé‡
    for i in range(len(original_encoder.norm_layers_1)):
        # è¿ç§» norm_layers_1
        original_norm1 = original_encoder.norm_layers_1[i]
        new_norm1 = new_encoder.norm_layers_1[i]
        if hasattr(original_norm1, 'gamma') and original_norm1.gamma is not None:
            new_norm1.gamma.data = original_norm1.gamma.data.to(DEVICE)
        if hasattr(original_norm1, 'beta') and original_norm1.beta is not None:
            new_norm1.beta.data = original_norm1.beta.data.to(DEVICE)

        # è¿ç§» norm_layers_2
        original_norm2 = original_encoder.norm_layers_2[i]
        new_norm2 = new_encoder.norm_layers_2[i]
        if hasattr(original_norm2, 'gamma') and original_norm2.gamma is not None:
            new_norm2.gamma.data = original_norm2.gamma.data.to(DEVICE)
        if hasattr(original_norm2, 'beta') and original_norm2.beta is not None:
            new_norm2.beta.data = original_norm2.beta.data.to(DEVICE)

        # è¿ç§» FFN å±‚
        new_encoder.ffn_layers[i].conv_1.weight.data = original_encoder.ffn_layers[i].conv_1.weight.data.to(DEVICE)
        new_encoder.ffn_layers[i].conv_2.weight.data = original_encoder.ffn_layers[i].conv_2.weight.data.to(DEVICE)
        if original_encoder.ffn_layers[i].conv_1.bias is not None:
            new_encoder.ffn_layers[i].conv_1.bias.data = original_encoder.ffn_layers[i].conv_1.bias.data.to(DEVICE)
        if original_encoder.ffn_layers[i].conv_2.bias is not None:
            new_encoder.ffn_layers[i].conv_2.bias.data = original_encoder.ffn_layers[i].conv_2.bias.data.to(DEVICE)

    return new_encoder

# ===================== åŠ è½½é…ç½® + æ„å»ºæ¨¡å‹ =====================
def load_config(config_path):
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)
    return config

def build_original_and_new_encoder(config, device):
    """
    æ„å»ºåŸå§‹Encoderå’Œä½ç§©Encoder
    """
    hps = config
    model_cfg = hps["model"]
    hidden_channels = model_cfg["hidden_channels"]
    filter_channels = model_cfg["filter_channels"]
    n_heads = model_cfg["n_heads"]
    n_layers = model_cfg["n_layers"]
    kernel_size = model_cfg["kernel_size"]
    p_dropout = model_cfg["p_dropout"]
    window_size = model_cfg.get("window_size", 4)

    # 1. å¯¼å…¥åŸå§‹æ¨¡å‹
    import attentions_original as attentions_ori
    original_encoder = attentions_ori.Encoder(
        hidden_channels=hidden_channels,
        filter_channels=filter_channels,
        n_heads=n_heads,
        n_layers=n_layers,
        kernel_size=kernel_size,
        p_dropout=p_dropout,
        window_size=window_size
    ).to(device)

    # 2. æ„å»ºä½ç§©Encoder
    new_encoder = Encoder(
        hidden_channels=hidden_channels,
        filter_channels=filter_channels,
        n_heads=n_heads,
        n_layers=n_layers,
        kernel_size=kernel_size,
        p_dropout=p_dropout,
        window_size=window_size
    ).to(device)

    return original_encoder, new_encoder

def load_original_model_weights(original_encoder, config, ckpt_path, device):
    """åŠ è½½åŸå§‹é¢„è®­ç»ƒæƒé‡åˆ°åŸå§‹Encoder"""
    if not os.path.exists(ckpt_path):
        raise FileNotFoundError(f"åŸå§‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼š{ckpt_path}")
    ckpt = torch.load(ckpt_path, map_location=device, weights_only=True)

    # æå–TextEncoderä¸­çš„encoderæƒé‡
    encoder_state_dict = {}
    if "net_g" in ckpt:
        if "enc_p" in ckpt["net_g"]:
            enc_p_dict = ckpt["net_g"]["enc_p"]
            for k, v in enc_p_dict.items():
                if k.startswith("encoder."):
                    new_k = k.replace("encoder.", "", 1)
                    encoder_state_dict[new_k] = v
    else:
        encoder_state_dict = ckpt.get("encoder", {})

    original_encoder.load_state_dict(encoder_state_dict, strict=False)
    print("åŸå§‹Encoderæƒé‡åŠ è½½æˆåŠŸï¼")
    return original_encoder

# ===================== ä¸»è¿ç§»æµç¨‹ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰ =====================
if __name__ == "__main__":
    try:
        # 1. åŠ è½½é…ç½®
        print("="*50 + " åŠ è½½é…ç½®æ–‡ä»¶ " + "="*50)
        config = load_config(CONFIG_PATH)
        print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸï¼š{CONFIG_PATH}")

        # 2. æ„å»ºåŸå§‹Encoderå’Œä½ç§©Encoder
        print("\n" + "="*50 + " æ„å»ºåŸå§‹æ¨¡å‹å’Œä½ç§©æ¨¡å‹ " + "="*50)
        original_encoder, new_encoder = build_original_and_new_encoder(config, DEVICE)
        print("åŸå§‹Encoderå’Œä½ç§©Encoderæ„å»ºæˆåŠŸï¼")

        # 3. åŠ è½½åŸå§‹é¢„è®­ç»ƒæƒé‡åˆ°åŸå§‹Encoder
        print("\n" + "="*50 + " åŠ è½½åŸå§‹é¢„è®­ç»ƒæƒé‡ " + "="*50)
        original_encoder = load_original_model_weights(original_encoder, config, ORIGINAL_CKPT_PATH, DEVICE)

        # 4. è¿ç§»æƒé‡ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰
        print("\n" + "="*50 + " è¿ç§»æ³¨æ„åŠ›å±‚æƒé‡ï¼ˆSVDåˆ†è§£ï¼‰ " + "="*50)
        new_encoder = transfer_encoder_weights(original_encoder, new_encoder, RANK)
        print("æ‰€æœ‰æ³¨æ„åŠ›å±‚æƒé‡è¿ç§»å®Œæˆï¼")

        # ğŸ”´ ä¿®å¤1ï¼šå…ˆéªŒè¯ä½ç§©Encoderæ˜¯å¦åŒ…å«W1/W2å‚æ•°
        print("\n" + "="*50 + " éªŒè¯ä½ç§©Encoderå‚æ•° " + "="*50)
        low_rank_params = []
        for name, param in new_encoder.named_parameters():
            if "W1" in name or "W2" in name:
                low_rank_params.append((name, param.shape))
        if len(low_rank_params) == 0:
            raise ValueError("âŒ ä½ç§©Encoderä¸­æ— W1/W2å‚æ•°ï¼attentions.pyå®šä¹‰é”™è¯¯ï¼")
        else:
            print(f"âœ… ä½ç§©EncoderåŒ…å« {len(low_rank_params)} ä¸ªW1/W2å‚æ•°ï¼š")
            for name, shape in low_rank_params:
                print(f"  {name} | {shape}")

        # 5. æ„å»ºå®Œæ•´æ¨¡å‹ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šå…ˆæ„å»ºç©ºæ¨¡å‹ï¼Œå†æ›¿æ¢Encoderï¼Œæœ€ååŠ è½½åŸå§‹æƒé‡ï¼‰
        print("\n" + "="*50 + " æ„å»ºå®Œæ•´ä½ç§©æ¨¡å‹å¹¶ä¿å­˜æƒé‡ " + "="*50)
        hps = config
        model_cfg = hps["model"]
        
        # æ„å»ºç©ºçš„SynthesizerTrnæ¨¡å‹
        new_model = SynthesizerTrn(
            n_vocab=178,  # æ›¿æ¢ä¸ºä½ å®é™…çš„n_vocabï¼ˆå¯ä»config["data"]["n_vocab"]è·å–ï¼‰
            spec_channels=hps["data"]["filter_length"] // 2 + 1,
            segment_size=hps["train"]["segment_size"] // hps["data"]["hop_length"],
            inter_channels=model_cfg["inter_channels"],
            hidden_channels=model_cfg["hidden_channels"],
            filter_channels=model_cfg["filter_channels"],
            n_heads=model_cfg["n_heads"],
            n_layers=model_cfg["n_layers"],
            kernel_size=model_cfg["kernel_size"],
            p_dropout=model_cfg["p_dropout"],
            resblock=model_cfg["resblock"],
            resblock_kernel_sizes=model_cfg["resblock_kernel_sizes"],
            resblock_dilation_sizes=model_cfg["resblock_dilation_sizes"],
            upsample_rates=model_cfg["upsample_rates"],
            upsample_initial_channel=model_cfg["upsample_initial_channel"],
            upsample_kernel_sizes=model_cfg["upsample_kernel_sizes"],
            n_speakers=model_cfg.get("n_speakers", 0),
            gin_channels=model_cfg.get("gin_channels", 0),
        ).to(DEVICE)


        # åŠ è½½åŸå§‹å®Œæ•´æ¨¡å‹æƒé‡ï¼ˆåªåŠ è½½éEncoderéƒ¨åˆ†ï¼‰
        original_ckpt = torch.load(ORIGINAL_CKPT_PATH, map_location=DEVICE, weights_only=True)
        model_state_dict = original_ckpt.get("net_g", original_ckpt)
        
        # è¿‡æ»¤æ‰enc_p.encoderçš„æƒé‡ï¼Œé¿å…è¦†ç›–ä½ç§©å±‚
        filtered_original_dict = {}
        for k, v in model_state_dict.items():
            if not k.startswith("enc_p.encoder."):
                filtered_original_dict[k] = v
        
        # åŠ è½½è¿‡æ»¤åçš„åŸå§‹æƒé‡ï¼ˆåªåŠ è½½éEncoderéƒ¨åˆ†ï¼‰
        new_model.load_state_dict(filtered_original_dict, strict=False)
        new_model.enc_q.pre = torch.nn.Conv1d(80, 192, 7, padding=3).to(DEVICE)
        # ğŸ”´ ä¿®å¤2ï¼šå…ˆæ›¿æ¢ä½ç§©Encoderï¼Œå†åŠ è½½åŸå§‹æƒé‡ï¼ˆé¿å…è¦†ç›–ï¼‰
        new_model.enc_p.encoder = new_encoder
        # ğŸ”´ ä¿®å¤3ï¼šéªŒè¯å®Œæ•´æ¨¡å‹æ˜¯å¦åŒ…å«W1/W2å‚æ•°
        print("\n" + "="*50 + " éªŒè¯å®Œæ•´æ¨¡å‹å‚æ•° " + "="*50)
        full_model_low_rank_params = []
        for name, param in new_model.named_parameters():
            if "W1" in name or "W2" in name:
                full_model_low_rank_params.append((name, param.shape))
        if len(full_model_low_rank_params) == 0:
            raise ValueError("âŒ å®Œæ•´æ¨¡å‹ä¸­æ— W1/W2å‚æ•°ï¼æ›¿æ¢Encoderå¤±è´¥ï¼")
        else:
            print(f"âœ… å®Œæ•´æ¨¡å‹åŒ…å« {len(full_model_low_rank_params)} ä¸ªW1/W2å‚æ•°ï¼š")
            for name, shape in full_model_low_rank_params:
                print(f"  {name} | {shape}")
        for name, p in new_model.named_parameters():
            if "W1" in name or "W2" in name:
                p.requires_grad_(True)
        # x = torch.randn(2, 50, 192).to(DEVICE)
        # y = new_encoder.attn_layers[0].conv_q(x)
        # print(y.shape) 
        # ---------- æœ€å°å¯å¤ç°ï¼šæ£€æŸ¥ä½ç§©å±‚æ˜¯å¦èƒ½æ­£å¸¸å‡ºæ¢¯åº¦ ----------
        new_model.train()
        B, T_txt = 1, 50
        T_mel = 200          # éšä¾¿ > T_txt å³å¯
        print(new_model.enc_q.pre)
        x = torch.randint(0, 178, (B, T_txt)).to(DEVICE)
        x_lengths = torch.LongTensor([T_txt]).to(DEVICE)
        y = torch.randn(B, 80, T_mel).to(DEVICE)  # 80 ç»´æ¢…å°”
        y_lengths = torch.LongTensor([T_mel]).to(DEVICE)


# 2. å† forward &  backward
        outs = new_model(x, x_lengths, y, y_lengths)
        (z, m_p, logs_p, z_mask, y_hat, ids_slice, attn) = outs
        loss =  z.mean()
        loss.backward()
        
        # 3. æœ€åæµ‹æ¢¯åº¦
        pq = new_model.enc_p.encoder.attn_layers[0].conv_q
        print('W1 data_ptr:', pq.W1.weight.data_ptr())   # çœŸå®å†…å­˜åœ°å€
        print('W1 id:', id(pq.W1.weight))       
        print('W1 grad:', pq.W1.weight.grad.abs().mean().item() if pq.W1.weight.grad is not None else 'None')
        # 6. ä¿å­˜è¿ç§»åçš„æƒé‡
        new_ckpt = {
            "net_g": new_model.state_dict(),
            "config": config,
            "low_rank_r": RANK,
            "transfer_method": "SVDå¥‡å¼‚å€¼åˆ†è§£",
            "original_ckpt": ORIGINAL_CKPT_PATH
        }
        torch.save(new_ckpt, NEW_INIT_CKPT_PATH)
        print(f"\nâœ… ä½ç§©æ¨¡å‹åˆå§‹åŒ–æƒé‡å·²ä¿å­˜ï¼š{NEW_INIT_CKPT_PATH}")

        # 7. éªŒè¯å‚æ•°é‡å¯¹æ¯”
        print("\n" + "="*50 + " éªŒè¯æƒé‡è¿ç§»æ•ˆæœ " + "="*50)
        def count_params(model):
            return sum(p.numel() for p in model.parameters()) / 1e6

        original_encoder_params = count_params(original_encoder)
        new_encoder_params = count_params(new_encoder)
        param_reduction = ((original_encoder_params - new_encoder_params) / original_encoder_params) * 100

        print(f"åŸå§‹Encoderå‚æ•°é‡ï¼š{original_encoder_params:.4f} M")
        print(f"ä½ç§©Encoderå‚æ•°é‡ï¼š{new_encoder_params:.4f} M")
        print(f"å‚æ•°é‡å‡å°‘æ¯”ä¾‹ï¼š{param_reduction:.2f}%")
        print("="*50 + " æƒé‡è¿ç§»å…¨éƒ¨å®Œæˆï¼ " + "="*50)

    except Exception as e:
        print(f"\nâŒ æƒé‡è¿ç§»å¤±è´¥ï¼š{e}")
        raise e