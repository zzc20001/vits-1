import os
import torch
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import json
import logging

# ===================== å…¨å±€é…ç½®ä¸Žæ—¥å¿— =====================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ===================== æ ¸å¿ƒé…ç½®ï¼ˆè¯·ç¡®è®¤è·¯å¾„æ­£ç¡®æ€§ï¼‰ =====================
LOW_RANK_CKPT_PATH = "./init_low_rank_model.pth"  # è¿ç§»åŽçš„ä½Žç§©æƒé‡è·¯å¾„
FREEZE_NON_LOW_RANK = True                        # ä»…è®­ç»ƒä½Žç§©å±‚
TRAIN_LR = 1e-5                                   # ä½Žç§©å±‚å¾®è°ƒå­¦ä¹ çŽ‡
EXPECTED_LOW_RANK_PARAMS = 54                     # é¢„æœŸçš„ä½Žç§©å‚æ•°æ•°é‡ï¼ˆè¿ç§»è¾“å‡ºä¸º54ï¼‰

# ===================== å¯¼å…¥æ ¸å¿ƒæ¨¡å—ï¼ˆå…³é”®ï¼šå¯¼å…¥ä½Žç§©Encoderï¼‰ =====================
try:
    from utils import get_hparams
    from models import SynthesizerTrn
    from text import symbols
    # å¯¼å…¥ä½Žç§©ç‰ˆæœ¬çš„Encoderï¼ˆattentions.pyï¼‰å’ŒåŽŸå§‹ç‰ˆæœ¬ï¼ˆç”¨äºŽå¯¹æ¯”ï¼‰
    from attentions import Encoder as LowRankEncoder
    import attentions_original as attentions_ori
except ImportError as e:
    logger.error(f"æ¨¡å—å¯¼å…¥å¤±è´¥ï¼š{str(e)}")
    raise ImportError("è¯·ç¡®è®¤attentions.py/attentions_original.py/models.pyè·¯å¾„æ­£ç¡®")

# ===================== åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ– =====================
def init_distributed(args):
    if 'LOCAL_RANK' in os.environ:
        args.local_rank = int(os.environ['LOCAL_RANK'])
    torch.cuda.set_device(args.local_rank)
    dist.init_process_group(backend='nccl')
    args.world_size = dist.get_world_size()
    args.rank = dist.get_rank()
    return args

# ===================== ä¸»è®­ç»ƒå‡½æ•°ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰ =====================
def run(rank, n_gpus, hps):
    """å•è¿›ç¨‹è®­ç»ƒé€»è¾‘ï¼ˆæ”¯æŒå•/å¤šGPUï¼‰"""
    # åˆ†å¸ƒå¼åˆå§‹åŒ–
    if n_gpus > 1:
        dist.init_process_group(
            backend='nccl',
            init_method='tcp://127.0.0.1:54321',
            world_size=n_gpus,
            rank=rank
        )
    
    # è®¾ç½®è®¾å¤‡
    torch.cuda.set_device(rank)
    device = torch.device(f"cuda:{rank}")
    logger.info(f"[Rank {rank}] ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # ===================== 1. æž„å»ºåŸºç¡€æ¨¡åž‹ =====================
    logger.info(f"[Rank {rank}] æž„å»ºSynthesizerTrnåŸºç¡€æ¨¡åž‹...")
    net_g = SynthesizerTrn(
        len(symbols),
        hps.data.filter_length // 2 + 1,
        hps.train.segment_size // hps.data.hop_length,
        **hps.model
    ).to(device)

    # ===================== 2. å¼ºåˆ¶æ›¿æ¢ä¸ºä½Žç§©Encoderï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰ =====================
    logger.info(f"[Rank {rank}] æ›¿æ¢enc_p.encoderä¸ºä½Žç§©ç‰ˆæœ¬...")
    # æå–Encoderé…ç½®å‚æ•°ï¼ˆä¸Žæƒé‡è¿ç§»ä»£ç å®Œå…¨ä¸€è‡´ï¼‰
    model_cfg = hps.model

    encoder_config = {
        "hidden_channels": model_cfg["hidden_channels"],
        "filter_channels": model_cfg["filter_channels"],
        "n_heads": model_cfg["n_heads"],
        "n_layers": model_cfg["n_layers"],
        "kernel_size": model_cfg["kernel_size"],
        "p_dropout": model_cfg["p_dropout"],
        "window_size": getattr(model_cfg, "window_size", 4) 
    }

    # æž„å»ºä½Žç§©Encoderå¹¶æ›¿æ¢
    low_rank_encoder = LowRankEncoder(**encoder_config).to(device)
    net_g.enc_p.encoder = low_rank_encoder
    logger.info(f"[Rank {rank}] âœ… ä½Žç§©Encoderæ›¿æ¢å®Œæˆï¼Œç±»åž‹ï¼š{type(net_g.enc_p.encoder)}")

    # ===================== 3. åŠ è½½ä½Žç§©æƒé‡ï¼ˆç»“æž„åŒ¹é…ï¼‰ =====================
    if not os.path.exists(LOW_RANK_CKPT_PATH):
        raise FileNotFoundError(f"ä½Žç§©æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼š{LOW_RANK_CKPT_PATH}")
    
    try:
        logger.info(f"[Rank {rank}] åŠ è½½ä½Žç§©æƒé‡ï¼š{LOW_RANK_CKPT_PATH}")
        low_rank_ckpt = torch.load(
            LOW_RANK_CKPT_PATH,
            map_location=device,
            weights_only=True
        )
        net_g_state_dict = low_rank_ckpt.get("net_g", low_rank_ckpt)
        
        # åŠ è½½æƒé‡ï¼ˆstrict=False å¿½ç•¥è§£ç å™¨ç­‰ä¸åŒ¹é…å‚æ•°ï¼‰
        net_g.load_state_dict(net_g_state_dict, strict=False)
        logger.info(f"[Rank {rank}] âœ… ä½Žç§©æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"[Rank {rank}] æƒé‡åŠ è½½å¤±è´¥ï¼š{str(e)}")
        raise

    # ===================== 4. éªŒè¯ä½Žç§©å‚æ•°åŠ è½½ç»“æžœï¼ˆå…³é”®æ£€æŸ¥ï¼‰ =====================
    if rank == 0:
        logger.info("\n[Rank 0] ===== éªŒè¯ä½Žç§©å‚æ•°åŠ è½½ç»“æžœ =====")
        low_rank_params = []
        for name, param in net_g.named_parameters():
            if "W1" in name or "W2" in name:
                low_rank_params.append((name, param.shape))
                logger.info(f"âœ… åŠ è½½å‚æ•°ï¼š{name} | å½¢çŠ¶ï¼š{param.shape}")
        
        # æ•°é‡æ ¡éªŒ
        param_count = len(low_rank_params)
        logger.info(f"\n[Rank 0] æ€»è®¡åŠ è½½ä½Žç§©å‚æ•°ï¼š{param_count} ä¸ªï¼ˆé¢„æœŸï¼š{EXPECTED_LOW_RANK_PARAMS} ä¸ªï¼‰")
        if param_count != EXPECTED_LOW_RANK_PARAMS:
            raise ValueError(
                f"ä½Žç§©å‚æ•°æ•°é‡ä¸åŒ¹é…ï¼åŠ è½½{param_count}ä¸ªï¼Œé¢„æœŸ{EXPECTED_LOW_RANK_PARAMS}ä¸ª\n"
                "è¯·æ£€æŸ¥ï¼š1.æƒé‡æ–‡ä»¶æ˜¯å¦æ­£ç¡® 2.ä½Žç§©Encoderæ›¿æ¢æ˜¯å¦æˆåŠŸ"
            )
        logger.info("[Rank 0] =========================")

    # ===================== 5. å‚æ•°å†»ç»“ç­–ç•¥ï¼ˆç²¾å‡†åŒ¹é…ä½Žç§©å±‚ï¼‰ =====================
    if FREEZE_NON_LOW_RANK:
        logger.info(f"[Rank {rank}] æ‰§è¡Œå‚æ•°å†»ç»“ç­–ç•¥...")
        frozen_params = 0
        trainable_params = 0

        # ç²¾å‡†åŒ¹é…ä½Žç§©å±‚å‚æ•°ï¼ˆä¸Žæƒé‡è¿ç§»çš„å‚æ•°åå®Œå…¨å¯¹é½ï¼‰
        for name, param in net_g.named_parameters():
            # åŒ¹é…è§„åˆ™ï¼šenc_p.encoder.attn_layers + conv_q/k/v + W1/W2
            if (
                "enc_p.encoder.attn_layers" in name
                and ("conv_q" in name or "conv_k" in name or "conv_v" in name)
                and ("W1" in name or "W2" in name)
            ):
                param.requires_grad = True
                trainable_params += param.numel()
                if rank == 0:
                    logger.info(f"[Rank 0] å¯è®­ç»ƒå‚æ•°ï¼š{name} | å‚æ•°é‡ï¼š{param.numel():,}")
            else:
                param.requires_grad = False
                frozen_params += param.numel()

        # ç»Ÿè®¡è¾“å‡º
        if rank == 0:
            logger.info(f"\n[Rank 0] âœ… å‚æ•°å†»ç»“å®Œæˆï¼š")
            logger.info(f"  å¯è®­ç»ƒå‚æ•°ï¼š{trainable_params/1e6:.4f} M")
            logger.info(f"  å†»ç»“å‚æ•°ï¼š{frozen_params/1e6:.4f} M")
            if trainable_params == 0:
                raise ValueError("æ— å¯ç”¨çš„å¯è®­ç»ƒå‚æ•°ï¼è¯·æ£€æŸ¥å‚æ•°åŒ¹é…è§„åˆ™")

    # é¢å¤–å†»ç»“è§£ç å™¨ï¼ˆç¡®ä¿ä¸è®­ç»ƒï¼‰
    for p in net_g.dec.parameters():
        p.requires_grad = False
    if rank == 0:
        logger.info("[Rank 0] âœ… è§£ç å™¨å·²å¼ºåˆ¶å†»ç»“")

    # ===================== 6. ä¼˜åŒ–å™¨åˆå§‹åŒ– =====================
    # èŽ·å–å¯è®­ç»ƒå‚æ•°åˆ—è¡¨
    trainable_params_list = list(filter(lambda p: p.requires_grad, net_g.parameters()))
    
    if rank == 0:
        logger.info(f"\n[Rank 0] åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼Œå¯è®­ç»ƒå‚æ•°æ•°é‡ï¼š{len(trainable_params_list)}")
    
    # ç©ºå‚æ•°æ£€æŸ¥
    if len(trainable_params_list) == 0:
        raise ValueError(f"[Rank {rank}] ä¼˜åŒ–å™¨æ— å¯ç”¨å‚æ•°ï¼")

    # åˆå§‹åŒ–AdamWä¼˜åŒ–å™¨
    optim_g = torch.optim.AdamW(
        trainable_params_list,
        lr=TRAIN_LR,
        betas=hps.train.betas if hasattr(hps.train, 'betas') else (0.8, 0.99),
        eps=hps.train.eps if hasattr(hps.train, 'eps') else 1e-9,
        weight_decay=hps.train.weight_decay if hasattr(hps.train, 'weight_decay') else 0.0
    )
    logger.info(f"[Rank {rank}] âœ… AdamWä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼ˆå­¦ä¹ çŽ‡ï¼š{TRAIN_LR}ï¼‰")

    # ===================== 7. åˆ†å¸ƒå¼æ¨¡åž‹åŒ…è£… =====================
    if n_gpus > 1:
        net_g = DDP(net_g, device_ids=[rank])
        logger.info(f"[Rank {rank}] âœ… åˆ†å¸ƒå¼æ¨¡åž‹åŒ…è£…å®Œæˆ")

    # ===================== 8. è®­ç»ƒå¾ªçŽ¯ï¼ˆæ›¿æ¢ä¸ºä½ çš„ä¸šåŠ¡é€»è¾‘ï¼‰ =====================
    logger.info(f"\n[Rank {rank}] âœ… æ¨¡åž‹åˆå§‹åŒ–å…¨éƒ¨å®Œæˆï¼Œå¼€å§‹ä½Žç§©å±‚å¾®è°ƒè®­ç»ƒï¼")
    
    try:
        # è®­ç»ƒå‚æ•°é…ç½®
        epochs = hps.train.epochs if hasattr(hps.train, 'epochs') else 100
        eval_interval = hps.train.eval_interval if hasattr(hps.train, 'eval_interval') else 10
        save_dir = hps.model_dir if hasattr(hps, 'model_dir') else "./checkpoints"
        os.makedirs(save_dir, exist_ok=True)

        # ä¸»è®­ç»ƒå¾ªçŽ¯
        for epoch in range(1, epochs + 1):
            logger.info(f"\n[Rank {rank}] ===== Epoch {epoch}/{epochs} =====")
            
            # è®­ç»ƒæ¨¡å¼
            net_g.train()
            
            # ===================== æ›¿æ¢ä¸ºä½ çš„çœŸå®žè®­ç»ƒé€»è¾‘ =====================
            # ä»¥ä¸‹æ˜¯ç¤ºä¾‹æ¡†æž¶ï¼Œéœ€æ›¿æ¢ä¸ºå®žé™…çš„æ•°æ®åŠ è½½å’Œå‰å‘/åå‘ä¼ æ’­
            # 1. æ•°æ®åŠ è½½ç¤ºä¾‹ï¼š
            # for batch_idx, batch in enumerate(train_dataloader):
            #     x, x_lengths, y, y_lengths = [b.to(device) for b in batch]
            #     
            #     # 2. å‰å‘ä¼ æ’­
            #     y_hat, l_lengths, attn, *_ = net_g(x, x_lengths, y, y_lengths)
            #     
            #     # 3. æŸå¤±è®¡ç®—
            #     loss = compute_loss(y_hat, y, y_lengths, l_lengths)
            #     
            #     # 4. åå‘ä¼ æ’­
            #     optim_g.zero_grad()
            #     loss.backward()
            #     optim_g.step()
            #     
            #     # 5. æ—¥å¿—è¾“å‡º
            #     if rank == 0 and batch_idx % 10 == 0:
            #         logger.info(f"Batch {batch_idx} | Loss: {loss.item():.4f}")
            # =================================================================

            # æƒé‡ä¿å­˜ï¼ˆæ¯eval_intervalä¸ªepochä¿å­˜ä¸€æ¬¡ï¼‰
            if rank == 0 and epoch % eval_interval == 0:
                save_path = os.path.join(save_dir, f"G_epoch_{epoch}.pth")
                save_dict = {
                    "net_g": net_g.module.state_dict() if n_gpus > 1 else net_g.state_dict(),
                    "optim_g": optim_g.state_dict(),
                    "epoch": epoch,
                    "hps": hps,
                    "low_rank_config": encoder_config
                }
                torch.save(save_dict, save_path)
                logger.info(f"[Rank 0] âœ… æƒé‡å·²ä¿å­˜ï¼š{save_path}")

        # è®­ç»ƒå®Œæˆä¿å­˜æœ€ç»ˆæƒé‡
        if rank == 0:
            final_save_path = os.path.join(save_dir, "G_final.pth")
            torch.save({
                "net_g": net_g.module.state_dict() if n_gpus > 1 else net_g.state_dict(),
                "optim_g": optim_g.state_dict(),
                "epochs": epochs,
                "train_config": {"lr": TRAIN_LR}
            }, final_save_path)
            logger.info(f"[Rank 0] âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæƒé‡ä¿å­˜ï¼š{final_save_path}")

    except KeyboardInterrupt:
        logger.info(f"[Rank {rank}] è®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­")
        # ä¸­æ–­æ—¶ä¿å­˜ä¸´æ—¶æƒé‡
        if rank == 0:
            interrupt_save_path = os.path.join(save_dir, "G_interrupt.pth")
            torch.save({
                "net_g": net_g.module.state_dict() if n_gpus > 1 else net_g.state_dict(),
                "optim_g": optim_g.state_dict()
            }, interrupt_save_path)
            logger.info(f"[Rank 0] âœ… ä¸­æ–­æƒé‡å·²ä¿å­˜ï¼š{interrupt_save_path}")
    except Exception as e:
        logger.error(f"[Rank {rank}] è®­ç»ƒå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        raise

# ===================== ç¨‹åºå…¥å£ =====================
if __name__ == "__main__":
    # åŠ è½½é…ç½®æ–‡ä»¶
    logger.info("ðŸ“Œ åŠ è½½è®­ç»ƒé…ç½®æ–‡ä»¶...")
    hps = get_hparams()
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    n_gpus = torch.cuda.device_count()
    logger.info(f"ðŸ“Œ ç³»ç»ŸGPUæ•°é‡ï¼š{n_gpus}")
    logger.info(f"ðŸ“Œ ä½Žç§©æƒé‡è·¯å¾„ï¼š{LOW_RANK_CKPT_PATH}")
    logger.info(f"ðŸ“Œ æ¨¡åž‹ä¿å­˜ç›®å½•ï¼š{hps.model_dir if hasattr(hps, 'model_dir') else './checkpoints'}")

    # å¯åŠ¨è®­ç»ƒï¼ˆå•/å¤šGPUé€‚é…ï¼‰
    if n_gpus > 1:
        mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hps))
    else:
        run(0, 1, hps)